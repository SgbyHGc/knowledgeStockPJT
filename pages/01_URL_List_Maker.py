# -*- coding: utf-8 -*-
import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time

def crawl_web_pages(url, pattern, max_depth=2):
  """
  Crawls a given URL recursively to extract unique links to web pages
  matching a specific pattern.

  This function starts at a given URL and explores linked pages up to a
  specified depth, collecting only the URLs that match the provided pattern
  and lead to web pages (HTML documents). It avoids revisiting the same URLs
  to prevent infinite loops.

  Args:
    url: The URL to start crawling.
    pattern: A string representing the pattern to match in the URLs (e.g., "/blog/").
    max_depth: The maximum depth to crawl.

  Returns:
    A list of unique URLs matching the pattern and leading to web pages.
  """
  visited_urls = set()
  urls = []

  def crawl(url, depth):
    """
    Performs the recursive crawling.

    Args:
      url: The URL to crawl.
      depth: The current depth of the crawl.
    """
    if depth > max_depth:
      return

    if url in visited_urls:
      return
    visited_urls.add(url)

    try:
      response = requests.get(url, timeout=10)
      response.raise_for_status()
      time.sleep(3)

      # Check if the response is an HTML document
      if 'text/html' not in response.headers['content-type']:
        return

    except requests.exceptions.RequestException as e:
      print(f"Error fetching {url}: {e}")
      return

    soup = BeautifulSoup(response.content, "html.parser")

    # Check if the current URL matches the pattern
    if pattern in urlparse(url).path:
      urls.append(url)
      print(f"Found URL: {url}")

    for link in soup.find_all("a", href=True):
      absolute_url = urljoin(url, link["href"])

      # Check if the link leads to a web page (not an image, PDF, etc.)
      parsed_link = urlparse(absolute_url)
      if not parsed_link.path.endswith(('.jpg', '.jpeg', '.png', '.gif', '.pdf', '.zip', '.rar')):
        crawl(absolute_url, depth + 1)

  crawl(url, 1)
  return urls

def get_title_from_url(url):
  """
  Fetches the title of a web page given its URL.

  Args:
    url: The URL of the web page.

  Returns:
    The title of the web page (str).
  """
  try:
    response = requests.get(url)
    response.raise_for_status()  # Raise an exception for error status codes
    soup = BeautifulSoup(response.content, "html.parser")
    title = soup.find("title").text
    return title
  except requests.exceptions.RequestException as e:
    print(f"Failed to fetch the page: {e}")
    return None
  except Exception as e:
    print(f"Failed to extract the title: {e}")
    return None

def download_urls(urls):
    if not urls:
        st.warning("Please select at least one URL.")
        return

    text_content = "\n".join(urls)
    filename = "selected_urls.txt"
    st.download_button(
        label="é¸æŠã—ãŸURLã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=f,
        file_name=filename,
        mime="text/plain",
    )


# Streamlitã‚¢ãƒ—ãƒªã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¨­å®š
st.title("URLãƒªã‚¹ãƒˆä½œæˆ ğŸ“")
st.markdown('---')
st.markdown("""
URLã®ãƒšãƒ¼ã‚¸ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ãƒªãƒ³ã‚¯ã‚’è¾¿ã£ã¦URLã®ãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚
æŒ‡å®šã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹URLã®ã¿ã‚’ãƒªã‚¹ãƒˆåŒ–ã—ã¾ã™ã€‚ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãªã©ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚
æ·±åº¦ã¯ã€ãƒªãƒ³ã‚¯å…ˆã®ãƒªãƒ³ã‚¯ã®æ·±ã•ã‚’ç¤ºã—ã¾ã™ã€‚ãƒªãƒ³ã‚¯å…ˆã®ãƒªãƒ³ã‚¯å…ˆã®ãƒªãƒ³ã‚¯ã¾ã§åé›†ã™ã‚‹å ´åˆã¯3ã€‚
""")
st.markdown('---')

# Streamlitã®å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
start_url = st.text_input('URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„', value='https://www.thinkwithgoogle.com/intl/ja-jp/marketing-strategies/')
url_pattern = st.text_input('ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„', value='/marketing-strategies/')
max_depth = st.number_input('æœ€å¤§æ·±åº¦ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„', min_value=1, max_value=3, value=2)

if "checked_urls" not in st.session_state:
    st.session_state.checked_urls = set()

if st.button("Search"):
    urls = crawl_web_pages(start_url, url_pattern, max_depth)

    if urls:
        with st.form("url_form"):
            selected_urls = []
            for url in urls:
              
                checked = st.checkbox(url, key=url, value=(url in st.session_state.checked_urls))
                st.write(checked)
                st.write(url)
                if checked:  # ãƒã‚§ãƒƒã‚¯ã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿URLã‚’è¿½åŠ 
                    selected_urls.append(url)

            submitted = st.form_submit_button("é¸æŠã—ãŸURLã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
            if submitted:
                st.write(f"DEBUG: st.session_state.checked_urls before update: {st.session_state.checked_urls}") # æ›´æ–°å‰ã®çŠ¶æ…‹
                st.session_state.checked_urls = set(selected_urls)
                st.write(f"DEBUG: st.session_state.checked_urls after update: {st.session_state.checked_urls}") # æ›´æ–°å¾Œã®çŠ¶æ…‹
                download_selected_urls(selected_urls)