# -*- coding: utf-8 -*-
import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
# import hashlib
import time
# import io

def get_title_from_url(url):
  """
  Fetches the title of a web page given its URL.

  Args:
    url: The URL of the web page.

  Returns:
    The title of the web page (str).
  """
  try:
    response = requests.get(url, timeout=10)
    response.raise_for_status()
    soup = BeautifulSoup(response.content, "html.parser")
    return soup.find("title").text
  except requests.exceptions.RequestException as e:
    st.error(f"Failed to fetch the page: {url} - {e}")
  except Exception as e:
    st.error(f"Failed to extract the title: {url} - {e}")
  return None

def download_selected_urls(selected_urls):
  if not selected_urls:
    st.error("No URLs selected for download.")
    return

  st.download_button(
    label="é¸æŠã—ãŸURLã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data="\n".join(selected_urls).encode('utf-8'),
    file_name="selected_urls.txt",
    mime="text/plain",
    on_click=lambda: st.write("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™...")
  )

def crawl_web_pages(start_url, pattern, max_depth=2):
  visited_urls = st.session_state.visited_urls
  matched_urls = set()
  
  def _crawl(url, depth, progress_bar, progress_text):
    if depth > max_depth or url in visited_urls:
      return

    visited_urls.add(url)
    st.session_state.visited_urls = visited_urls

    try:
      response = requests.get(url, timeout=10)
      response.raise_for_status()
      time.sleep(3)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ã‚’èª¿æ•´

      # HTMLä»¥å¤–ã¯ã‚¹ã‚­ãƒƒãƒ—
      if response.headers.get('content-type', '').lower().find('text/html') == -1:
        return
        
      soup = BeautifulSoup(response.content, "html.parser")

      # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒç¢ºèª
      if pattern in url:
        matched_urls.add(url)

      # é€²è¡ŒçŠ¶æ³æ›´æ–°
      progress_bar.progress(min(1.0, len(visited_urls) / (max_depth * 50)))
      progress_text.text(f"å‡¦ç†æ¸ˆã¿URLæ•°: {len(visited_urls)} (è©²å½“URLæ•°: {len(matched_urls)})")

      # å†å¸°çš„ã«ãƒªãƒ³ã‚¯ã‚’ãŸã©ã‚‹
      for link in soup.find_all("a", href=True):
        absolute_url = urljoin(url, link["href"])
        # ä¸è¦ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã¨è¨ªå•æ¸ˆã¿URLã‚’é™¤å¤–
        if not any(absolute_url.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif', '.pdf', '.zip', '.rar']):
          _crawl(absolute_url, depth + 1, progress_bar, progress_text)
    except requests.exceptions.RequestException as e:
      st.error(f"ã‚¨ãƒ©ãƒ¼: {url} - {e}")

  # é€²è¡ŒçŠ¶æ³è¡¨ç¤ºã®åˆæœŸåŒ–
  progress_bar = st.progress(0)
  progress_text = st.empty()
  progress_text.text("å‡¦ç†é–‹å§‹...")

  # ã‚¯ãƒ­ãƒ¼ãƒ«å®Ÿè¡Œ
  _crawl(start_url, 1, progress_bar, progress_text)

  # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
  progress_bar.progress(1.0)
  progress_text.text(f"å®Œäº†: ç·URLæ•° {len(visited_urls)} (è©²å½“URLæ•°: {len(matched_urls)})")

  return list(matched_urls)

# --- Streamlit UI ---
st.set_page_config(page_title="URLåé›†ã‚¢ãƒ—ãƒª")
st.title("URLåé›†ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ğŸ“")
st.write("""
URLã®ãƒšãƒ¼ã‚¸ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ãƒªãƒ³ã‚¯ã‚’è¾¿ã£ã¦URLã®ãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚\n
æŒ‡å®šã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹URLã®ã¿ã‚’ãƒªã‚¹ãƒˆåŒ–ã—ã¾ã™ã€‚ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãªã©ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚
æ·±åº¦ã¯ã€ãƒªãƒ³ã‚¯å…ˆã®ãƒªãƒ³ã‚¯ã®æ·±ã•ã‚’ç¤ºã—ã¾ã™ã€‚ãƒªãƒ³ã‚¯å…ˆã®ãƒªãƒ³ã‚¯å…ˆã®ãƒªãƒ³ã‚¯ã¾ã§åé›†ã™ã‚‹å ´åˆã¯3ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚
""")
st.write('---')

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
start_url = st.text_input("é–‹å§‹URL:", value="https://www.thinkwithgoogle.com/intl/ja-jp/marketing-strategies/")
url_pattern = st.text_input("URLãƒ‘ã‚¿ãƒ¼ãƒ³:", value="/marketing-strategies/")
max_depth = st.number_input("æœ€å¤§æ·±åº¦:", min_value=1, max_value=5, value=2)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'visited_urls' not in st.session_state:
    st.session_state.visited_urls = set()
if 'urls' not in st.session_state:
    st.session_state.urls = []

# ã‚¯ãƒ­ãƒ¼ãƒ«é–‹å§‹
if st.button("ã‚¯ãƒ­ãƒ¼ãƒ«é–‹å§‹"):
    st.session_state.visited_urls.clear()
    st.session_state.urls = []
    if start_url:
        st.session_state.urls = crawl_web_pages(start_url, url_pattern, max_depth)
    else:
        st.warning("é–‹å§‹URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

# çµæœè¡¨ç¤ºã¨é¸æŠ
if st.session_state.urls:
    st.write("---")
    selected_urls = []
    for i, url in enumerate(st.session_state.urls):
        title = get_title_from_url(url)
        # st.write(f"ã‚¿ã‚¤ãƒˆãƒ«: {title}")
        st.markdown(f"**{title}**")
        # ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã®åˆæœŸçŠ¶æ…‹ã‚’ç®¡ç†
        checkbox_key = f'checkbox_{i}'
        if checkbox_key not in st.session_state:
            st.session_state[checkbox_key] = True
        if st.checkbox(url, key=checkbox_key):
            selected_urls.append(url)
        st.markdown("<br>", unsafe_allow_html=True)
        # st.write("---")

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
    download_selected_urls(selected_urls)
