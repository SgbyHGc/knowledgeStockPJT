# -*- coding: utf-8 -*-
import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import hashlib
import time
import io

def crawl_web_pages(url, pattern, max_depth=2):
    """
    æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€ç‰¹å®šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ä¸€æ„ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã—ã¾ã™ã€‚

    ã“ã®é–¢æ•°ã¯ã€æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰é–‹å§‹ã—ã€æŒ‡å®šã•ã‚ŒãŸæ·±ã•ã¾ã§ãƒªãƒ³ã‚¯ã•ã‚ŒãŸãƒšãƒ¼ã‚¸ã‚’æ¢ç´¢ã—ã€
    æä¾›ã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã—ã€ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ï¼ˆHTMLãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰ã«ã¤ãªãŒã‚‹URLã®ã¿ã‚’åé›†ã—ã¾ã™ã€‚
    ç„¡é™ãƒ«ãƒ¼ãƒ—ã‚’é˜²ããŸã‚ã«ã€åŒã˜URLã¸ã®å†ã‚¢ã‚¯ã‚»ã‚¹ã¯é¿ã‘ã¾ã™ã€‚

    Args:
        url: ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’é–‹å§‹ã™ã‚‹URLã€‚
        pattern: URLã§ä¸€è‡´ã•ã›ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¡¨ã™æ–‡å­—åˆ—ï¼ˆä¾‹ï¼š "/blog/"ï¼‰ã€‚
        max_depth: ã‚¯ãƒ­ãƒ¼ãƒ«ã™ã‚‹æœ€å¤§æ·±åº¦ã€‚

    Returns:
        ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã—ã€ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã«ã¤ãªãŒã‚‹ä¸€æ„ã®URLã®ãƒªã‚¹ãƒˆã€‚
    """
    visited_urls = set()
    urls = []

    def crawl(url, depth):
        """
        å†å¸°çš„ãªã‚¯ãƒ­ãƒ¼ãƒ«ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

        Args:
            url: ã‚¯ãƒ­ãƒ¼ãƒ«ã™ã‚‹URLã€‚
            depth: ç¾åœ¨ã®ã‚¯ãƒ­ãƒ¼ãƒ«ã®æ·±åº¦ã€‚
        """
        if depth > max_depth:
            return

        if url in visited_urls:
            return
        visited_urls.add(url)

        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()  # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã€ä¾‹å¤–ã‚’ç™ºç”Ÿã•ã›ã‚‹
            time.sleep(3)  # ã‚µãƒ¼ãƒãƒ¼ã¸ã®è² è·ã‚’è»½æ¸›ã™ã‚‹ãŸã‚ã€å°‘ã—å¾…æ©Ÿ

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒHTMLãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
            content_type = response.headers.get('content-type')
            if content_type is None or 'text/html' not in content_type.lower():
                return

        except requests.exceptions.RequestException as e:
            st.error(f"{url} ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return

        soup = BeautifulSoup(response.content, "html.parser")

        # ç¾åœ¨ã®URLãŒãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ã‹ç¢ºèª
        if pattern in urlparse(url).path:
            urls.append(url)

        for link in soup.find_all("a", href=True):
            absolute_url = urljoin(url, link["href"])

            # ãƒªãƒ³ã‚¯ãŒã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ï¼ˆç”»åƒã€PDFãªã©ã§ã¯ãªã„ï¼‰ã«ã¤ãªãŒã‚‹ã“ã¨ã‚’ç¢ºèª
            parsed_link = urlparse(absolute_url)
            if not parsed_link.path.endswith(('.jpg', '.jpeg', '.png', '.gif', '.pdf', '.zip', '.rar')):
                crawl(absolute_url, depth + 1)

    crawl(url, 1)
    return urls


def download_urls(urls):
    data = "\n".join(urls).encode('utf-8')
    filename = "selected_urls.txt"
    st.download_button(
        label="é¸æŠã—ãŸURLã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=data,
        file_name=filename,
        mime="text/plain",
    )


# Streamlitã‚¢ãƒ—ãƒªã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¨­å®š
st.title("URLãƒªã‚¹ãƒˆä½œæˆ ğŸ“")
st.markdown('---')
st.markdown("""
URLã®ãƒšãƒ¼ã‚¸ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ãƒªãƒ³ã‚¯ã‚’è¾¿ã£ã¦URLã®ãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚
æŒ‡å®šã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹URLã®ã¿ã‚’ãƒªã‚¹ãƒˆåŒ–ã—ã¾ã™ã€‚ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãªã©ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚
æ·±åº¦ã¯ã€ãƒªãƒ³ã‚¯å…ˆã®ãƒªãƒ³ã‚¯ã®æ·±ã•ã‚’ç¤ºã—ã¾ã™ã€‚ãƒªãƒ³ã‚¯å…ˆã®ãƒªãƒ³ã‚¯å…ˆã®ãƒªãƒ³ã‚¯ã¾ã§åé›†ã™ã‚‹å ´åˆã¯3ã€‚
""")
st.markdown('---')

# Streamlitã®å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
start_url = st.text_input('URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„', value='https://www.thinkwithgoogle.com/intl/ja-jp/marketing-strategies/')
url_pattern = st.text_input('ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„', value='/marketing-strategies/')
max_depth = st.number_input('æœ€å¤§æ·±åº¦ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„', min_value=1, max_value=3, value=2)

if st.button("Search"):
    urls = crawl_web_pages(start_url, url_pattern, max_depth)
if urls is not None:
    if "selected_urls" not in st.session_state:
        st.session_state.selected_urls = []

    selected_urls = st.multiselect("URLã‚’é¸æŠ", urls, key="selected_urls")
    if selected_urls:
        download_urls(selected_urls)
    else:
        st.warning("URLãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
